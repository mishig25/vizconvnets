"use strict";
var __extends = (this && this.__extends) || (function () {
    var extendStatics = Object.setPrototypeOf ||
        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||
        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };
    return function (d, b) {
        extendStatics(d, b);
        function __() { this.constructor = d; }
        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());
    };
})();
Object.defineProperty(exports, "__esModule", { value: true });
var tfjs_core_1 = require("@tensorflow/tfjs-core");
var K = require("./backend/tfjs_backend");
var errors_1 = require("./errors");
var generic_utils_1 = require("./utils/generic_utils");
var LayersOptimizer = (function () {
    function LayersOptimizer(config) {
        if (config instanceof tfjs_core_1.Optimizer) {
            this.createdFromCoreOptimizer = true;
            this.constructFromCoreOptimizer(config);
        }
        else {
            this.createdFromCoreOptimizer = false;
            this.clipnorm = config.clipnorm;
            this.clipvalue = config.clipvalue;
            this.constructFromConfig(config);
        }
    }
    LayersOptimizer.prototype.getConfig = function () {
        if (this.createdFromCoreOptimizer) {
            throw new errors_1.NotImplementedError('getConfig() for a LayersOptimizer constructed from a core ' +
                'Optimizer is not supported yet.');
        }
        var config = {};
        if (this.clipnorm != null) {
            config['clipnorm'] = this.clipnorm;
        }
        if (this.clipvalue != null) {
            config['clipvalue'] = this.clipvalue;
        }
        return config;
    };
    LayersOptimizer.prototype.updateVariables = function (lossFn, params) {
        var variables = params.map(function (param) { return param.read(); });
        return this.optimizer.minimize(lossFn, true, variables);
    };
    LayersOptimizer.fromConfig = function (cls, config) {
        return new cls(config);
    };
    return LayersOptimizer;
}());
exports.LayersOptimizer = LayersOptimizer;
var SGD = (function (_super) {
    __extends(SGD, _super);
    function SGD(config) {
        return _super.call(this, config) || this;
    }
    SGD.prototype.constructFromConfig = function (config) {
        this.lr = (config.lr == null) ? 0.01 : config.lr;
        if (this.lr < 0) {
            throw new errors_1.ValueError("Invalid lr (" + this.lr + "). Must be >= 0 or undefined.");
        }
        this.momentum = (config.momentum == null) ? 0.0 : config.momentum;
        if (this.momentum < 0) {
            throw new errors_1.ValueError("Invalid momentum (" + this.momentum + "). Must be >= 0 or undefined.");
        }
        if (this.momentum !== 0) {
            throw new errors_1.NotImplementedError('SGD momentum is not implemented yet.');
        }
        this.decay = (config.decay == null) ? 0.0 : config.decay;
        if (this.decay < 0) {
            throw new errors_1.ValueError("Invalid decay (" + this.decay + "). Must be >= 0 or undefined.");
        }
        if (this.decay !== 0) {
            throw new errors_1.NotImplementedError('SGD decay is not implemented yet');
        }
        this.nesterov = (config.nesterov == null) ? false : config.nesterov;
        if (this.nesterov !== false) {
            throw new errors_1.NotImplementedError('SGD nesterov is not implemented yet');
        }
        this.optimizer = tfjs_core_1.train.sgd(this.lr);
    };
    SGD.prototype.constructFromCoreOptimizer = function (optimizer) {
        if (!(optimizer instanceof tfjs_core_1.SGDOptimizer)) {
            throw new errors_1.ValueError('Cannot construct SGD from a non-SGD core optimizer');
        }
        this.optimizer = optimizer;
    };
    SGD.prototype.getConfig = function () {
        var config = {
            lr: this.lr,
            momentum: this.momentum,
            decay: this.decay,
            nestorv: this.nesterov,
        };
        var baseConfig = _super.prototype.getConfig.call(this);
        Object.assign(config, baseConfig);
        return config;
    };
    return SGD;
}(LayersOptimizer));
exports.SGD = SGD;
generic_utils_1.ClassNameMap.register('SGD', SGD);
var Adam = (function (_super) {
    __extends(Adam, _super);
    function Adam(config) {
        return _super.call(this, config) || this;
    }
    Adam.prototype.constructFromConfig = function (config) {
        this.lr = config.lr == null ? 0.001 : config.lr;
        this.beta1 = config.beta_1 == null ? 0.9 : config.beta_1;
        this.beta2 = config.beta_2 == null ? 0.999 : config.beta_2;
        this.epsilon = config.epsilon == null ? K.epsilon() : config.epsilon;
        this.decay = config.decay == null ? 0 : config.decay;
        if (this.decay !== 0.0) {
            throw new errors_1.NotImplementedError('Adam decay is not implemented yet');
        }
        this.amsgrad = config.amsgrad == null ? false : config.amsgrad;
        if (this.amsgrad !== false) {
            throw new errors_1.NotImplementedError('Adam amsgrad is not implemented yet');
        }
        this.optimizer = tfjs_core_1.train.adam(this.lr, this.beta1, this.beta2, this.epsilon);
    };
    Adam.prototype.constructFromCoreOptimizer = function (optimizer) {
        if (!(optimizer instanceof tfjs_core_1.AdamOptimizer)) {
            throw new errors_1.ValueError('Cannot construct Adam from a non-Adam core optimizer');
        }
        this.optimizer = optimizer;
    };
    Adam.prototype.getConfig = function () {
        var config = {
            lr: this.lr,
            beta1: this.beta1,
            beta2: this.beta2,
            decay: this.decay,
            epsilon: this.epsilon,
            amsgrad: this.amsgrad
        };
        var baseConfig = _super.prototype.getConfig.call(this);
        Object.assign(config, baseConfig);
        return config;
    };
    return Adam;
}(LayersOptimizer));
exports.Adam = Adam;
generic_utils_1.ClassNameMap.register('Adam', Adam);
var RMSProp = (function (_super) {
    __extends(RMSProp, _super);
    function RMSProp(config) {
        return _super.call(this, config) || this;
    }
    RMSProp.prototype.constructFromConfig = function (config) {
        this.lr = config.lr == null ? 0.001 : config.lr;
        this.rho = config.rho == null ? 0.9 : config.rho;
        this.epsilon = config.epsilon == null ? K.epsilon() : config.epsilon;
        if (config.decay != null) {
            throw new errors_1.NotImplementedError('RMSProp decay is not implemented yet');
        }
        this.optimizer = tfjs_core_1.train.rmsprop(this.lr, this.rho, null, this.epsilon);
    };
    RMSProp.prototype.constructFromCoreOptimizer = function (optimizer) {
        if (!(optimizer instanceof tfjs_core_1.RMSPropOptimizer)) {
            throw new errors_1.ValueError('Cannot construct RMSProp from a non-RMSProp core optimizer');
        }
        this.optimizer = optimizer;
    };
    RMSProp.prototype.getConfig = function () {
        var config = {
            lr: this.lr,
            rho: this.rho,
            decay: this.decay,
            epsilon: this.epsilon,
        };
        var baseConfig = _super.prototype.getConfig.call(this);
        Object.assign(config, baseConfig);
        return config;
    };
    return RMSProp;
}(LayersOptimizer));
exports.RMSProp = RMSProp;
generic_utils_1.ClassNameMap.register('RMSProp', RMSProp);
var Adagrad = (function (_super) {
    __extends(Adagrad, _super);
    function Adagrad(config) {
        return _super.call(this, config) || this;
    }
    Adagrad.prototype.constructFromConfig = function (config) {
        this.lr = config.lr == null ? 0.01 : config.lr;
        this.epsilon = config.epsilon == null ? K.epsilon() : config.epsilon;
        this.decay = config.decay == null ? 0 : config.decay;
        if (this.decay !== 0) {
            throw new errors_1.NotImplementedError('Adagrad decay is not implemented yet');
        }
        this.optimizer = tfjs_core_1.train.adagrad(this.lr);
    };
    Adagrad.prototype.constructFromCoreOptimizer = function (optimizer) {
        if (!(optimizer instanceof tfjs_core_1.AdagradOptimizer)) {
            throw new errors_1.ValueError('Cannot construct Adagrad from a non-Adagrad core optimizer');
        }
        this.optimizer = optimizer;
    };
    Adagrad.prototype.getConfig = function () {
        var config = {
            lr: this.lr,
            decay: this.decay,
            epsilon: this.epsilon,
        };
        var baseConfig = _super.prototype.getConfig.call(this);
        Object.assign(config, baseConfig);
        return config;
    };
    return Adagrad;
}(LayersOptimizer));
exports.Adagrad = Adagrad;
generic_utils_1.ClassNameMap.register('Adagrad', Adagrad);
exports.adagrad = Adagrad;
exports.adam = Adam;
exports.rmsprop = RMSProp;
exports.sgd = SGD;
function get(identifier) {
    var coreOptimizerToConstructorMap = {
        'AdagradOptimizer': Adagrad,
        'AdamOptimizer': Adam,
        'RMSPropOptimizer': RMSProp,
        'SGDOptimizer': SGD
    };
    var optimizerMap = { Adagrad: Adagrad, Adam: Adam, RMSProp: RMSProp, SGD: SGD, adagrad: exports.adagrad, adam: exports.adam, rmsprop: exports.rmsprop, sgd: exports.sgd };
    if (typeof identifier === 'string') {
        if (identifier in optimizerMap) {
            return optimizerMap[identifier];
        }
        throw new errors_1.ValueError("Unknown Optimizer " + identifier);
    }
    else {
        var coreOptimizerTypeName = identifier.constructor.name;
        if (coreOptimizerTypeName in coreOptimizerToConstructorMap) {
            return coreOptimizerToConstructorMap[coreOptimizerTypeName];
        }
        throw new errors_1.ValueError("Unsupported core optimizer type: " + coreOptimizerTypeName);
    }
}
exports.get = get;
